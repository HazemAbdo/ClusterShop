{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:21:08 WARN Utils: Your hostname, khaldon-LENOVO resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp0s20f3)\n",
      "23/04/15 15:21:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/15 15:21:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ClusterShop\").getOrCreate()\n",
    "\n",
    "# Read the Excel file using Pandas\n",
    "df_pandas = pd.read_excel(\"Online Retail.xlsx\",na_values='')\n",
    "\n",
    "# Convert the Pandas DataFrame to PySpark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:23:55 WARN TaskSetManager: Stage 0 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:23:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/04/15 15:23:58 WARN TaskSetManager: Stage 1 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|        UnitPrice|CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "|  count|            541909|            541909|              541909|            541909|           541909|    541909|     541909|\n",
      "|   mean|  559965.752026781|27623.240210938104|                 NaN|  9.55224954743324|4.611113626088897|       NaN|       null|\n",
      "| stddev|13428.417280798658| 16799.73762842766|                 NaN|218.08115785023472|96.75985306117991|       NaN|       null|\n",
      "|    min|            536365|             10002| 4 PURPLE FLOCK D...|            -80995|        -11062.06|   12346.0|  Australia|\n",
      "|    max|           C581569|                 m|   wrongly sold sets|             80995|          38970.0|       NaN|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe the DataFrame\n",
    "df_spark.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:24:05 WARN TaskSetManager: Stage 4 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/15 15:24:10 WARN TaskSetManager: Stage 5 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# count the number of missing values in the CustomerID column\n",
    "df_spark.rdd.map(lambda x: 1 if math.isnan( x['CustomerID'])  else 0).sum()\n",
    "\n",
    "# filter out the missing values using map \n",
    "df_spark_filtered = df_spark.rdd.map(lambda x: x if not math.isnan( x['CustomerID'])  else None).filter(lambda x: x is not None).toDF()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:24:10 WARN TaskSetManager: Stage 6 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+-----------------+------------------+------------------+-----------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|         Quantity|         UnitPrice|        CustomerID|    Country|\n",
      "+-------+-----------------+------------------+--------------------+-----------------+------------------+------------------+-----------+\n",
      "|  count|           406829|            406829|              406829|           406829|            406829|            406829|     406829|\n",
      "|   mean|560617.1266447864|27430.341352504624|                null|12.06130339774205|3.4604710185376426|15287.690570239585|       null|\n",
      "| stddev|13106.16769474081|16403.570452626223|                null|248.6933700188252| 69.31516172321443|1713.6003033215966|       null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|           -80995|               0.0|           12346.0|  Australia|\n",
      "|    max|          C581569|              POST|ZINC WIRE SWEETHE...|            80995|           38970.0|           18287.0|Unspecified|\n",
      "+-------+-----------------+------------------+--------------------+-----------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark_filtered.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the InvoiceDate column to a date\n",
    "from pyspark.sql.functions import to_date\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"InvoiceDate\", to_date(df_spark_filtered.InvoiceDate, 'MM/dd/yyyy'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:24:26 WARN TaskSetManager: Stage 9 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|TotalCost|InvoiceMonth|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|   17850.0|United Kingdom|     15.3|          12|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|          12|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|   17850.0|United Kingdom|     22.0|          12|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|          12|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|          12|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2| 2010-12-01|     7.65|   17850.0|United Kingdom|     15.3|          12|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6| 2010-12-01|     4.25|   17850.0|United Kingdom|     25.5|          12|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6| 2010-12-01|     1.85|   17850.0|United Kingdom|     11.1|          12|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6| 2010-12-01|     1.85|   17850.0|United Kingdom|     11.1|          12|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32| 2010-12-01|     1.69|   13047.0|United Kingdom|    54.08|          12|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|   13047.0|United Kingdom|     12.6|          12|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|   13047.0|United Kingdom|     12.6|          12|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8| 2010-12-01|     3.75|   13047.0|United Kingdom|     30.0|          12|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6| 2010-12-01|     1.65|   13047.0|United Kingdom|      9.9|          12|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6| 2010-12-01|     4.25|   13047.0|United Kingdom|     25.5|          12|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3| 2010-12-01|     4.95|   13047.0|United Kingdom|    14.85|          12|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2| 2010-12-01|     9.95|   13047.0|United Kingdom|     19.9|          12|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3| 2010-12-01|     5.95|   13047.0|United Kingdom|    17.85|          12|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3| 2010-12-01|     5.95|   13047.0|United Kingdom|    17.85|          12|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4| 2010-12-01|     7.95|   13047.0|United Kingdom|     31.8|          12|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "# create a new column called TotalCost\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"TotalCost\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# round the TotalCost column to 2 decimal places\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"TotalCost\", round(df_spark_filtered.TotalCost, 2))\n",
    "\n",
    "# create a new column called InvoiceMonth\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"InvoiceMonth\", month(df_spark_filtered.InvoiceDate))\n",
    "\n",
    "\n",
    "df_spark_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/15 15:24:28 WARN TaskSetManager: Stage 10 contains a task of very large size (8485 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+---------------------+\n",
      "|CustomerID|InvoiceMonth|TotalPurchases|      TotalRevenue|AvgRevenuePerPurchase|\n",
      "+----------+------------+--------------+------------------+---------------------+\n",
      "|   17850.0|          12|           297| 5391.210000000009|    5391.210000000009|\n",
      "|   16098.0|          12|            12|430.59999999999997|   430.59999999999997|\n",
      "|   18074.0|          12|            13|             489.6|                489.6|\n",
      "|   16250.0|          12|            14|226.14000000000001|   226.14000000000001|\n",
      "|   15862.0|          12|           111| 527.3799999999999|    527.3799999999999|\n",
      "|   16218.0|          12|            28|471.29999999999995|   471.29999999999995|\n",
      "|   12838.0|          12|            59|390.78999999999985|   390.78999999999985|\n",
      "|   13758.0|          12|            24|            718.85|               718.85|\n",
      "|   13694.0|          12|            70|            6016.2|               6016.2|\n",
      "|   16210.0|          12|            49|10600.039999999999|   10600.039999999999|\n",
      "|   14078.0|          12|            13|            136.24|               136.24|\n",
      "|   12662.0|          12|            28|486.43000000000006|   486.43000000000006|\n",
      "|   15350.0|          12|             5|115.64999999999999|   115.64999999999999|\n",
      "|   15922.0|          12|            12| 363.6000000000001|    363.6000000000001|\n",
      "|   14594.0|          12|            53|            379.83|               379.83|\n",
      "|   17346.0|          12|            78|384.92999999999995|   384.92999999999995|\n",
      "|   16274.0|          12|            67|357.94999999999993|   357.94999999999993|\n",
      "|   17690.0|          12|            26| 687.9000000000002|    687.9000000000002|\n",
      "|   14142.0|          12|            22|            311.81|               311.81|\n",
      "|   14606.0|          12|           333|1581.4700000000012|   1581.4700000000012|\n",
      "+----------+------------+--------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import count, mean, sum\n",
    "\n",
    "# Define a function to aggregate the data by customer ID and invoice month\n",
    "def aggregate_data_by_customer(row):\n",
    "    # Extract the customer ID and invoice month from the row\n",
    "    customerID, invoiceMonth, invoiceNo, totalCost = row[\"CustomerID\"], row[\"InvoiceMonth\"], row[\"InvoiceNo\"], row[\"TotalCost\"]\n",
    "    # Convert the total cost to a float\n",
    "    totalCost = float(totalCost)\n",
    "    # Return a tuple containing the customer ID, invoice month, and a dictionary with the aggregated values\n",
    "    return ((customerID, invoiceMonth), {\"TotalPurchases\": 1, \"TotalRevenue\": totalCost, \"AvgRevenuePerPurchase\": totalCost})\n",
    "\n",
    "# Define a function to combine the aggregated data for each customer\n",
    "def combine_customer_data(data1, data2):\n",
    "    # Combine the dictionaries for the two sets of aggregated data\n",
    "    combined_data = {k: data1.get(k, 0) + data2.get(k, 0) for k in set(data1) | set(data2)}\n",
    "    # Return the combined data\n",
    "    return combined_data\n",
    "\n",
    "# Map the data using the aggregate_data_by_customer() function\n",
    "mapped_data = df_spark_filtered.rdd.map(aggregate_data_by_customer)\n",
    "\n",
    "# Reduce the data using the combine_customer_data() function\n",
    "reduced_data = mapped_data.reduceByKey(combine_customer_data)\n",
    "\n",
    "# Convert the reduced data to a DataFrame\n",
    "df_customer = reduced_data.map(lambda x: (x[0][0], x[0][1], x[1][\"TotalPurchases\"], x[1][\"TotalRevenue\"], x[1][\"AvgRevenuePerPurchase\"])).toDF([\"CustomerID\", \"InvoiceMonth\", \"TotalPurchases\", \"TotalRevenue\", \"AvgRevenuePerPurchase\"])\n",
    "\n",
    "# show the data frame   \n",
    "df_customer.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
