{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 16:16:58 WARN Utils: Your hostname, khaldon-LENOVO resolves to a loopback address: 127.0.1.1, but we couldn't find any external IP address!\n",
      "23/04/17 16:16:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/17 16:17:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/17 16:17:03 WARN MacAddressUtil: Failed to find a usable hardware address from the network interfaces; using random bytes: 20:9a:f3:07:3b:65:00:8d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ClusterShop\").getOrCreate()\n",
    "\n",
    "# Read the Excel file using Pandas\n",
    "df_pandas = pd.read_excel(\"Online Retail.xlsx\",na_values='')\n",
    "\n",
    "# Convert the Pandas DataFrame to PySpark DataFrame \n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level to Error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "df_spark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|        UnitPrice|CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "|  count|            541909|            541909|              541909|            541909|           541909|    541909|     541909|\n",
      "|   mean|  559965.752026781|27623.240210938104|                 NaN|  9.55224954743324|4.611113626088897|       NaN|       null|\n",
      "| stddev|13428.417280798658| 16799.73762842766|                 NaN|218.08115785023472|96.75985306117991|       NaN|       null|\n",
      "|    min|            536365|             10002| 4 PURPLE FLOCK D...|            -80995|        -11062.06|   12346.0|  Australia|\n",
      "|    max|           C581569|                 m|   wrongly sold sets|             80995|          38970.0|       NaN|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+-----------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe the DataFrame\n",
    "df_spark.describe().show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|   541909|   541909|     540455|  541909|   541909|    406829| 541909|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|        0|        0|       1454|       0|        0|    135080|      0|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# percentage of missing values in each column of the DataFrame\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "# get columns excluding 'InvoiceDate'\n",
    "cols = [col for col in df_spark.columns if col != 'InvoiceDate']\n",
    "\n",
    "\n",
    "# count missing values in each column as a percentage of the total number of values and total number of values in each column\n",
    "df_spark.select([count(when(~isnan(c), c)).alias(c) for c in cols]).show()\n",
    "df_spark.select([count(when(isnan(c), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "|        0|        0|          0|       0|        0|         0|      0|\n",
      "+---------+---------+-----------+--------+---------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# filter out the missing values using map \n",
    "df_spark_filtered = df_spark.rdd.map(lambda x: x if not math.isnan(x['CustomerID'])  else None).filter(lambda x: x is not None).toDF()\n",
    "df_spark_filtered.select([count(when(isnan(c), c)).alias(c) for c in cols]).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Cancelled Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of cancelled invoices:  16.466876971608833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_invoice = df_spark_filtered.select('InvoiceNo').distinct().count()\n",
    "cancelled_invoice_count = df_spark_filtered.filter(df_spark_filtered['InvoiceNo'].startswith('C')).select('InvoiceNo').distinct().count()\n",
    "\n",
    "# percentage of cancelled invoices\n",
    "print(\"Percentage of cancelled invoices: \", (cancelled_invoice_count/unique_invoice)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in orderd_cancelled:  8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "[Stage 268:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in normal_orders:  397924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all transactions with the same invoice number but starts with 'C'\n",
    "# such that it has the same customer ID and StockCode and abs(Quantity) is the same\n",
    "# and abs(UnitPrice) is the same\n",
    "\n",
    "orderd_cancelled = df_spark_filtered.filter(df_spark_filtered['InvoiceNo'].startswith('C')).select('InvoiceNo', 'CustomerID', 'StockCode', 'Quantity', 'UnitPrice').orderBy('InvoiceNo', 'CustomerID', 'StockCode', 'Quantity', 'UnitPrice')\n",
    "\n",
    "normal_orders = df_spark_filtered.filter(~df_spark_filtered['InvoiceNo'].startswith('C')).select('InvoiceNo', 'CustomerID', 'StockCode', 'Quantity', 'UnitPrice').orderBy('InvoiceNo', 'CustomerID', 'StockCode', 'Quantity', 'UnitPrice')\n",
    "\n",
    "# count the number of rows in each DataFrame\n",
    "print(\"Number of rows in orderd_cancelled: \", orderd_cancelled.count())\n",
    "print(\"Number of rows in normal_orders: \", normal_orders.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the InvoiceDate column to a date\n",
    "from pyspark.sql.functions import to_date\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"InvoiceDate\", to_date(df_spark_filtered.InvoiceDate, 'MM/dd/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|TotalCost|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|   17850.0|United Kingdom|     15.3|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|   17850.0|United Kingdom|     22.0|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|    20.34|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2| 2010-12-01|     7.65|   17850.0|United Kingdom|     15.3|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6| 2010-12-01|     4.25|   17850.0|United Kingdom|     25.5|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6| 2010-12-01|     1.85|   17850.0|United Kingdom|     11.1|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6| 2010-12-01|     1.85|   17850.0|United Kingdom|     11.1|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32| 2010-12-01|     1.69|   13047.0|United Kingdom|    54.08|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|   13047.0|United Kingdom|     12.6|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6| 2010-12-01|      2.1|   13047.0|United Kingdom|     12.6|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8| 2010-12-01|     3.75|   13047.0|United Kingdom|     30.0|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6| 2010-12-01|     1.65|   13047.0|United Kingdom|      9.9|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6| 2010-12-01|     4.25|   13047.0|United Kingdom|     25.5|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3| 2010-12-01|     4.95|   13047.0|United Kingdom|    14.85|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2| 2010-12-01|     9.95|   13047.0|United Kingdom|     19.9|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3| 2010-12-01|     5.95|   13047.0|United Kingdom|    17.85|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3| 2010-12-01|     5.95|   13047.0|United Kingdom|    17.85|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4| 2010-12-01|     7.95|   13047.0|United Kingdom|     31.8|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "# create a new column called TotalCost\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"TotalCost\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# round the TotalCost column to 2 decimal places\n",
    "df_spark_filtered = df_spark_filtered.withColumn(\"TotalCost\", round(df_spark_filtered.TotalCost, 2))\n",
    "df_spark_filtered.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+\n",
      "|CustomerID|InvoiceNo|         TotalCost|\n",
      "+----------+---------+------------------+\n",
      "|   12346.0|   541431|           77183.6|\n",
      "|   12346.0|  C541433|          -77183.6|\n",
      "|   12347.0|   537626|            711.79|\n",
      "|   12347.0|   542237|            475.39|\n",
      "|   12347.0|   573511|1294.3200000000002|\n",
      "|   12347.0|   549222|            636.25|\n",
      "|   12347.0|   556201|            382.52|\n",
      "|   12347.0|   562032|            584.91|\n",
      "|   12347.0|   581180|224.82000000000002|\n",
      "|   12348.0|   541998|227.43999999999997|\n",
      "|   12348.0|   539318| 892.8000000000001|\n",
      "|   12348.0|   548955|             367.0|\n",
      "|   12348.0|   568172|             310.0|\n",
      "|   12349.0|   577609|           1757.55|\n",
      "|   12350.0|   543037|334.40000000000003|\n",
      "|   12352.0|   567505|            366.25|\n",
      "|   12352.0|   547390|160.32999999999998|\n",
      "|   12352.0|   544156|296.49999999999994|\n",
      "|   12352.0|   568699|            266.25|\n",
      "|   12352.0|  C545329|            -463.8|\n",
      "+----------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Basket size\n",
    "basket_price = df_spark_filtered.groupBy('CustomerID','InvoiceNo').agg({'TotalCost': 'sum'}).withColumnRenamed('sum(TotalCost)', 'TotalCost').sort('CustomerID')\n",
    "basket_price.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze description Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/khaldon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/khaldon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the description column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# analyze the description column \n",
    "\n",
    "\n",
    "# This function takes as input the dataframe and analyzes the content of the Description column by performing the following operations:\n",
    "\n",
    "#     extract the names (proper, common) appearing in the products description\n",
    "#     for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n",
    "#     count the number of times each root appears in the dataframe\n",
    "#     when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def analyze_description(df):\n",
    "    # get the description column\n",
    "    description = df.select('Description').rdd.map(lambda x: x[0]).collect()\n",
    "    # tokenize the description column\n",
    "    tokens = [word_tokenize(x) for x in description if x is not None]\n",
    "    \n",
    "    # get nouns and proper nouns\n",
    "    tokens = [[x[0] for x in nltk.pos_tag(y) if x[1] == 'NN' or x[1] == 'NNP'] for y in tokens]\n",
    "    \n",
    "    # get the root of each word\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stems = [[stemmer.stem(x) for x in y] for y in tokens]\n",
    "    # create a dictionary that maps each root to the set of words associated with this root\n",
    "    stem_dict = {}\n",
    "    for i in range(len(stems)):\n",
    "        for j in range(len(stems[i])):\n",
    "            if stems[i][j] not in stem_dict:\n",
    "                stem_dict[stems[i][j]] = set()\n",
    "            stem_dict[stems[i][j]].add(tokens[i][j])\n",
    "    # count the number of times each root appears in the dataframe\n",
    "    stem_count = {}\n",
    "    for i in range(len(stems)):\n",
    "        for j in range(len(stems[i])):\n",
    "            if stems[i][j] not in stem_count:\n",
    "                stem_count[stems[i][j]] = 0\n",
    "            stem_count[stems[i][j]] += 1\n",
    "    # create a dictionary that maps each root to the shortest word associated with this root\n",
    "    stem_shortest = {}\n",
    "    for key in stem_dict:\n",
    "        stem_shortest[key] = min(stem_dict[key], key=len)\n",
    "    return stem_dict, stem_count, stem_shortest\n",
    "\n",
    "\n",
    "print(\"Analyzing the description column...\")\n",
    "stem_dict, stem_count, stem_shortest = analyze_description(df_spark_filtered)\n",
    "print(\"Done!\")\n",
    "\n",
    "# get the top 10 most frequent roots\n",
    "top_10 = sorted(stem_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 most frequent roots: \", top_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart of the roots and their frequency in stem_count\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the roots and their frequency in stem_count\n",
    "sorted_stem_count = sorted(stem_count.items(), key=lambda x: x[1], reverse=True)\n",
    "roots = [x[0] for x in sorted_stem_count]\n",
    "counts = [x[1] for x in sorted_stem_count]\n",
    "\n",
    "# plot the roots and their frequency in stem_count\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(roots, counts)\n",
    "plt.title('Top 10 most frequent roots')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Root')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
